# 强化学习:

在这篇文章中，我们提出了关于某些算法的分析结果，这些算法用于处理那些具有关联性的任务，意味着学习者需要执行输入到输出的映射，并且，除了一个有限的例外情况外，这些任务涉及即时强化，意味着提供给学习者的强化（即，回报）仅由最近的输入-输出对决定。

虽然延迟强化任务显然很重要，并且最近受到了应有的关注，但开发此类任务算法的常用方法是将即时强化学习者与基于时间差分方法的自适应预测器或评论家结合起来（Sutton, 1988）。Barto、Sutton和Anderson（1983年）以及Sutton（1984年）研究的演员-评论家算法显然是这种形式，Watkins（1989年；Barto、Sutton和Watkins，1990年）的Q学习算法也是如此。

我们在这里所做的进一步假设是，学习者的搜索行为，作为任何形式的强化学习算法中必不可少的组成部分，是通过学习者输入输出行为的随机性来提供的。虽然这是一种实现所需探索行为的常见方式，但值得注意的是，在某些情况下，其他策略有时也是可行的，包括系统性的搜索或一致选择看似最佳的替代方案。后一种策略适用于那些替代行动的优劣是通过总是过于乐观的估计来确定的情况，并且随着经验的积累变得更加现实，正如在A*搜索中发生的那样（Nilsson, 1980）。

此外，所有结果将在这里以连接主义网络的视角进行阐述，主要关注那些遵循或估计相关梯度的算法。尽管这类算法已知存在许多局限性，但研究它们仍有许多理由。首先，正如反向传播（leCun, 1985; Parker, 1985; Rumelhart, Hinton & Williams, 1986; Werbos, 1974）的经验所示，梯度似乎提供了一个强大且通用的启发式基础，用于生成通常易于实现且在许多情况下出人意料有效的算法。其次，当需要更复杂的算法时，梯度计算通常可以作为这些算法的核心。此外，就某些现有算法与这种梯度分析产生的算法相似的程度而言，我们对它们的理解可能会得到增强。

这里介绍的算法的另一个显著特性是，尽管它们大致可以被描述为统计上爬升适当的梯度，但它们设法做到这一点，无需显式计算这个梯度的估计值，甚至无需存储可以从中直接计算出这种估计值的信息。这就是为什么它们在标题中被称为简单的。或许一个更有信息量的形容词将是非基于模型的。本文的后面部分将进一步讨论这一点。

尽管我们在这里采用的是连接主义的视角，但应该指出，所进行的分析的某些方面直接适用于其他实现自适应输入输出映射的方式。将要展示的结果一般适用于任何其输入输出映射由参数化的输入控制分布函数组成的学习者，输出是随机生成的，相应的算法根据性能反馈修改学习者的分布函数。由于这里使用的是梯度方法，这些结果潜在适用性的唯一限制是必须满足某些明显的可微分条件。





除非另有说明，我们假设在整个过程中，学习代理是一个由几个单独单元组成的前馈网络，每个单元本身也是一个学习代理。我们首先做出额外的假设，即所有单元都以随机方式运行，但稍后考虑网络中存在确定性单元的情况也是有用的。网络通过接收来自环境的外部输入，将相应的活动通过网络传播，并将在其输出单元产生的活动发送到环境中进行评估。评估包括标量强化信号r，我们假设这个信号被广播到网络中的所有单元。此时，每个单元根据使用的特定学习算法进行适当的权重修改，然后循环重新开始。

我们使用的符号如下：让 Yi 表示网络中第 i 个单元的输出，让 xi 表示对该单元的输入模式。这个输入模式 Xi 是一个向量，其各个元素（通常表示为 xj）要么是网络中某些单元的输出（那些直接将输出发送到第 i 个单元的），要么是来自环境的某些输入（如果该单元恰好连接到环境，以至于它直接接收来自环境的输入）。然后，输出 Yi 是根据 xi 和输入线上的权重 w0 来抽取的分布。对于每个 i，让 wi 表示由所有权重 w/j 组成的权重向量。让 W 表示由网络中所有权重 wij 组成的权重矩阵。在更一般的设置中，wi 可以被视为第 i 个单元（或代理）的行为所依赖的所有参数的集合，而 W 是整个（或代理集合）的行为所依赖的参数的集合。



此外，for each i let *gi (~, wi,* xi) = *er {Yi* = ~ [ wi, xi},，这样gi就是确定Yi值的概率质量函数，作为单元及其输入参数的函数。（为了叙述方便，我们始终使用适用于可能输出值Yi为离散集的情况的术语和符号，但是当gi被视为相应的概率密度函数时，所得结果也适用于连续值单元。）由于向量wi包含了第i个单元输入输出行为所依赖的所有网络参数，我们同样可以定义gi为gi(~, wi, xi) = Pr{yi = ~IW, X/}。